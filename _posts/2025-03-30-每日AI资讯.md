摘要:DeepSeek与清华大学的研究者合作发布新论文，提出自我原则点评调优（SPCT）方法和元奖励模型（meta RM），显著提升了奖励模型在推理时的可扩展性。SPCT分为拒绝式微调和基于规则的在线强化学习两阶段，有效提高了GRM的质量和扩展性，使DeepSeek-GRM-27B在基准测试中表现优异。研究团队还探索了推理时扩展策略，通过生成奖励投票和元奖励模型引导投票，进一步提升了模型性能。实验证明，DeepSeek-GRM-27B在推理时扩展上的表现优于单纯扩大模型规模。
原文url:https://www.aibase.com/zh/news/16861